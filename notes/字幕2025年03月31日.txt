好各位各位老师大家好啊,各位学员,那个我们今天因为一些特殊情况啊,所以就今天一上午呢就改成了线上啊,呃那个这个但是即使线上,我想这个我们还是学员们还是这个热情的,非常饱满,我看这个线上也呃包括线下,应该大部分的学员也都来听了啊,那个我也简单的给那个李伟老师说下,我们那个讲习班呢,今年还是这个吸引了很多的感兴趣的老师啊,特别是外地的这个这个外外地的很多,这个包括我们西北地区的很多老师,和这个博士生研究生,来来来来来听啊,大家都是带有很大的热情啊,然后我也衷心的感谢那个立伟老师,能在宝贵的在繁忙的工作当中,抽出宝贵的时间来给我们来做这个分享,我简单的介绍一下啊,王老师啊,啊王老师实际上在这个呃机器学习,基础研究领域呢,是是应该是在全国做的最好的,这个中生代的学者之一了,那么大家应该对王老师都非常的了解啊,那么我就说他几几几几几个那个比较显著的,比较那个的标签啊,首先就是它是我们那个我们中国啊,入选那个AITENSWATCH啊的第一位,我们中国的青年学者,那么这个奖项呢是每两年颁发一次,然后在全世界颁给十个这个青年学者,那么我们中国机器学习领域,包括像李伟老师,李伟老师第一个啊,然后后来是朱军老师,包括于洋老师啊,应该是三位吧啊,应该是三位啊,然后那个李伟老师呢也曾经呃,当然在我们那个顶顶会顶刊,始终是这个,这个一直是在发表着非常有影影响力的工作,他的工作连续获得我们经济学三大会,ACCLEAR20232024的杰出论文,以及这个杰出论文的这个提名奖啊,这个是非常了不起的啊,然后那个另外呢他有一篇工作,是在IISML的2019年,是荣选荣,荣获当年的那个十个最具影响力的,影响力的论文之一啊,那么当然这这些都是外在的荣誉啊,其实立伟老师始终在我的心目中是踏踏实实的,这个做研究的一线的学者,始终都是呃在做最深刻的最前沿的研究,所以我们今天非常有幸请到王老师,来给我们做这个在大模型基础理论,基础方法方面的这样一个方面的一个分享啊,那么首先感谢恩,那么我们就接下来我们就把时间交给李伟老师,麻烦那个请让我们大家欢迎,立伟老师给我们做分享啊,啊谢谢这个孟老师的介绍,这个咳咳呃其实我的背景也是数学,我的博士是数学,所以呢嗯我我从了解到就今天的很多嗯,来甘雨这个讲七班的老师和同学们呢,也是数学背景啊,所以我觉得大家这个也比较亲切,然后这个一些思维方式呢也是比较类似的,也很很高兴啊,这个这个孟老师的邀请了呃,我想这样子不好意思,我我今天这个这个北京这边过敏比较严重啊,那个是这样,就今天这个呃,主题呢是大模型里面的这个思维链,但是呢我想呃就是因为我了解到各位呢,其实这次我们是一个五天的啊,这样的一个讲习班,其实是想把这个大模型的理论的方方面面,都给大家呢做一个分享,所以我还是想在最开始把大模型一些非常底层,非常整体的一些东西呢跟大家分享一下,然后呢后面再逐渐进入这个呃所谓的思维链,其实思维链是这个大模型做reasoning,就是做推理的一个环节,这个我今天呢除了思维链以外呢,后面还会咳从这个就是大模型怎么去做推理,现在其实主要是从这个强化学习,用强化学习的方式去做推理这两个层面吧,都给大家做一个分享啊,好那我们就进入这个正题啊,那关于大模型的这个呃一些进展,好像我就不用太多说了,可能在座的每一位都用过这个大模型啊,我主要给大家介绍一下,就是今天的大模型,实际上它分成了几个部分,从技术的角度,它有几个部分就看这张啊,这张图啊,其实大模型今天最主要的分为三个环节,第一个环节就叫做大模型的预训练,第二部分呢叫做模型的监督微调,第三部分呢叫做这个alignment,就是作为对其人类的偏好,就首先我们知道是有这三个部分,那么预训练呢是之前大家讲的比较多的,就是我用所谓这种啊无监督的方式,我就用无监督,用大量的这种网上的数据,包括语料,甚至包括一些图等等去做这种self supervised learning,然后去学习,这是预训练部分啊,咳最主要,那么监督微调呢,是说后面我针对一些特定的任务,然后我收集一些监督数据,然后去做特定的这样的训练,那么目标呢是把某些认的这个性能,在进一步的提升啊,这是第二步,第三步就是你训好了,这个在预训练做完了,监督微调也做完了之后,但是要考虑到什么呢,就是你的训练数据是海量,这里面什么数据都有,那么最后你要对齐人类偏好,那么举个例子就是其实有一些违规的,甚至是违法的一些东西,你不能随意的让大模型出现,这里面就有一个对齐的问题啊,那么这是第三部分,所以从技术上前面也讲,就是这个预训练监督微调对齐啊,这个其实是三个部分,那么从预训练的角度来讲,他最主要利用的方法就是所谓这个next token,Prediction,就是预测下一个字符,那比如说我现在有一段话是吧,嗯这个several famous people are是吧,其实是那后面哎这是前半句,那我把这前半句输入给模型,我让模型预测下一个词,下一个token应该是什么,那模型呢它可能就输出一个对下一个词的预测,那么这个时候我的语料,我的数据其实我是知道下一个词是什么,我就可以通过这个进行比对,那么如果正确,我就奖励他,如果错误就惩罚他,然后呢,我再把这个词预测出来的词加到这个模型啊,之前的输入的后面作为一个新的输入,再交给模型啊去进行预测,这样周而复始,这就是所谓next token prediction,那么所以今天大模型的预训练啊,总体上就是去做这样的一个next token prediction,而这个各种自然语言处理的任务,全部要转化成这种序列生成的任务啊,统一成一个序列生成的任务,不管它原始任务是什么,我都可以做这样的转化来作弊,所以现在预训练大模型呢,应该是最开始大模型成功啊,就像GET最早期的这个版本,从GPT开始到GPT2,就知道GP搬到到这个ChatGPT,这个大量大家都是做的这种大模型的预训练啊,next token prediction这样的方式,那么大模型出现之后,其实大家呢逐渐发现他的啊强大的能力,那么在强大的能力背后,后面大家也发现他的一些弱点,他的弱点也是今天我们后面主要要讨论的就是,其他的推理能力,其实是大模型的短板,特别是在这个早期的版本中,比如说ChatGPT刚出来的时候,大家后面会会看到,还有很多的推理的这样的一些缺陷啊,那么这个专门针对推理,所以后面做了很多工作,那也是今天呢我主要想跟大家啊,来一起分享的这个内容,那么其中一方一个工作,就是我要跟大家谈的这个啊思维列嗯,这样的一个方式就是思维链呢,后面我会具体跟大家讲,就它不是简单的拿拿这个模型,拿一个这个神经网络transformer,让他从输入到输出啊,就只是这么简单的做一下,而是呢用这个所谓叫做china fox思维LIAM的技术啊,拆解这个复杂问题,所以这里面呢这个reasoning,后来就变成了一个非常专门的啊,这样的一个大模型的问题,而且呢我们的模型,其实除了刚才说的预训练的这个模型,我们把预训练得到的模型叫做基座模型,然后呢我们在这个基座模型基础之上,还要再去训练这个所谓的推理模型,就专为推理,要加强推理的能力啊,所以这是推理模型,那么今天呢我主要跟大家讨论的都是这个推理,这方面的啊,这个内容,好那么这这个呢是给大家具体的来介绍一下,就是基座模型和推理模型,它们两者各自的特点是什么啊,基诺模型,那就是过去大家通常所谓的大模型,就参数量巨大啊,然后呢他可以处理大量的任务,他是用这个无监督预训练的方式做出来的,那么推理模型呢,就是我要针对相对要针对这种一类问题,因为在很多人对话的时候,其实他有一些问题他不需要推理,但是呢有一些问题啊,特别是比如跟数学相关,跟代码相关,跟科学相关,那你必须要做推理啊,所以这是这个要做推理模型,好,那么训练基座模型的训练范式,和推理模型的训练范式啊,这个其实是有一定的差异的,刚才已经谈到过了,就机构模型,其实最主要的是用海量的文本数据做预训练,嗯但是呢基座但是推理模型里没有这这一步,没有这种海量数据训练的问题啊,那么机构模型刚才说了,还有监监督微调,然后去做这个alignment人类对齐啊,这个做对齐,那么推理模型呢就是从这个基座模型出发,然后用这个呃所谓监督微调,然后呢现在主要是强化学习去提升推理,那么这里的代表就是deep thick的R1的版本,这就是一个非常典型的一种强化学习,提升推理能力的这样一个事情啊,嗯那后面呢今天后面这个有团队的这个呃,善子康姆士啊,来给大家来具体介绍,用RL来做这个推理啊,好那么前面呢是非常宏观的,看一下今天的大模型有哪些成分啊,一个基座模型,一个推理模型,那么它们各自的训练方式是有所不同,那我先来介绍一下这个,如果我们主要看基座模型的训练,那么从一个理论的角度来讲,它都有哪些理论问题,那么实际上我跟大家谈这个理论问题呢,还想做一个对比,就是人工智能,包括这个里面最基础的部分,我们叫做机器学习,其实它发展也有几十年的历史了,他的理论也研究了大概有40年以上的时间,其实在过去已经建立了一套比较自封的,这样的一套理论体系,但是呢我们会看到这种经典的嗯,机器学习理论和今天大模型时代所面临的问题,其实有一个非常大的反差,那么我在这儿呢就是想跟大家一起来看一看,所谓经典的机器学习理论,他是在研究什么事情,研究什么问题,而今天大毛新时代我们面临的是什么问题,我们面临的是什么困难,和经典的机器学习其实有一个很大的反差,我们今天真正大模型需要解决的又是什么问题,给我们提理论提出了什么新问题啊,这是我在这几页来想跟大家一起分享嗯,那么机器学习理论啊,如果我们只限制在这个基座模型这部分的话,其实我们面主要讨论的是三个理论问题,分别叫做expressiveness,就是表达能力,模型的表达能力和第二个优化和,第三个叫做泛化啊,我分别来介绍一下,这是它们是什么含义,那么模型的表达能力这几个事情呢,其实应该说是一个比较纯粹的数学上的事情,这个事情嗯,如果我们抛开呃今天的AI啊,包括今天抛开今天的大模型,这个所谓表达能力最早就是数学上提出的,这第一个最著名的结论,就是所谓stone verse trusting啊,也就是说用我用多项式,可以任意逼近这个连续函数在这个compact set上,所以这个结论那都是几百年前最经典的结论,他想说的是,如果我给你一个target function,那你的这个模型类里面,是不是一定可以找出一个啊,可以足够精确逼近target function这样的函数,那么storm struth定理,这个是用多项式,那后面比如说我还可以考虑这个浮力叶,三角函数,也可以去做这样的逼记,那么在今天在人工智能,在AI里面,大家其实考虑的最多的是所谓神经网络,那么这个事情在上世纪其实80年代末,90年代初就已经开始研究了,当时大家就已经啊很清楚神经网络啊,作为一个函数类,你就可以比对,比如说多项式函数和三角函数好,三角函数的级数是吧,那么神经网络这个函数类,它也具有这种所谓叫做universal approximation,这样的一种性质啊,这是呢经典的机器学习理论做的事情,当然这个是上世纪差不多90年代初做的事情,那么到了近10年,大家做的事情呢和过去会有所不同,比如说上世纪八十九十年代,这个研究神经网络的逼近,他其实考虑的都是非常浅层的神经网络,而宽度呢是非常非常宽的,实际上是没有没有一个bug,但是今天呢我们用的更多的是所谓,叫做深度神经网络,而宽度是有限的,那么在过去大概10年左右的时间,其实呃这个大家逐渐建立的是,如果我允许深度加深而宽度有限,这样的神经网络,这样的一个函数类,它还是不是具有universal approximation的性质啊,但是所有这些呢,我们都把它归于这种经典的机器学习理论,关于表达能力,也就是大家去探讨神经网络啊,这个函数类它的一个表达能力的强和弱,这是经典的,但是这个问题到了大模型时代以后,其实完全变化,为什么变化了,因为今天啊从我个人的观点,当在我的大模型的时代,如果要看表达能力,单纯看模型本身,也就单纯我们去看一个神经网络,这是远远不够的,为什么呢,因为今天我们的大模型啊,不仅仅是说用transformer,神经网络这样的一个模型,而且还在于我们用的是一种叫做自回归的方式,我们是用自回归这种方式,去使用这个神经网络啊,那么因为时间的关系呢,可能这一部分啊,关于子回归会在今天下午的课程里面啊,给大家更详细的去做介绍啊,除了自回归以外,还有一个更重要的,在大模型推理里面起到极其重要作用,就是今天也是今天讨论的一个主题,就是所谓这种叫做china fault思维链的过程,所以今天呢后面我们会看到,你在谈大模型的时候,绝不仅仅是说我这儿有一个函数,绝不仅仅说我这儿有一个神经网络,把它看作一个函数,而是看作一个过程,就是它是一个模型,比如说一个神经网络是一个模型,但同时我还有使用的过程,包括自回归,包括channel f,去看表达能力啊,这也是今天去研究表达能力,跟经典的这个universal approximation,非常不一样的地方啊,这是最新的问题,好这个是关于表达能力,那么啊第二部分呢是所谓这个优化啊,当然优化呢也是在这个operation research这个领域里面,那大家就专门研究的这个问题嗯,那么从经典的机器学习啊,或者说经典的这个优化理论,大家研究什么呢,那经典的优化其实就只有两个问题,第一个问题是convergence,就是你做优化它收敛不收敛,第二个问题是如果收敛,那么我去问你的rate of converence,就是你的收敛速度有多快,就这两个问题,当然在经典的这个优化里面,大家可能更主要的考虑的是凸优化啊,如果是非凸的话,这个问题其实是很难回答的,但是到了今天大模型时代,刚才所说的经典优化理论里面问的两个问题,convergence和read convergence,实际上几乎变成了无关紧要的问题,这是令人非常震惊的,为什么会几乎变成了无关紧要的问题,阴天,因为今天在大模型训练的过程当中,我们从来不会训练到收敛,今天的大模型由于模型非常巨大,由于数据量非常巨大,今天我们的训练啊,大家知道大模型的训练其实就是一个优化过程,就是我给定一个啊objective function,给定一个损失函数,然后我在训练数据上,希望去minimize这个损失函数啊,这就是我们大模型的训练,它就是一个优化的这样的一个过程,那么今天的大模型的训练,是从来没有任何一个大模型会训练到收敛,那么都是用一种什么方式呢,是用一种叫做few pass training,他什么意思呢,就是我的训练数据可能一共就只过了三四遍,甚至在一些比较特殊的情况,我数据量特别大的时候,我的数据就只过了一遍,那个时候叫one pass training,那也就是你的训练training as这样的vision啊,所以既然我的训练就把数据只过了很少的几遍,他根本就没有到,或者甚至是接近convergence,那么过去做这个研究的convergence和rof convergence,这个研究的意义就非常的小,所以在大模型时代,我们必须要面临这样的新问题,好那么第三个这个机器学习的理论问题,那么经典的理论问题是什么样,首先经典的理论问题它只针对一个任务,比如说一个分类任务,一个classification的任务,或者一个regression的任务,我就是有一类这样的一个这样的问题,和相应的这一类数据,而且呢我们还要假定这一类这个数据啊,训练数据和测试数据都是独立同分布的,这是过去所谓这种经典的这种vision,那么所谓这样vision的意思就是说你的训练过程,这个优化过程都只看到的是训练数据,你所有的指标能观测到的指标,都是在训练数据上观测的,但是你真正所需要的是在未来还没有看到的,这种测试数据上的性能,才是你真正关心的真正需要的,那么测试性能和训练性能,可能二者之间会有一个gap,这就叫做generation,我怎么能让这个GA尽可能的小,这是我要做的关于general vation研究和分析啊,所以前面讲的这个经典的理论,general vision理论啊,核心就是它是对一个特定的任务,并且数据是独立同分布,那么到了今天大模型时代,这个JAVIATION泛化和经理发生了根本性的变化,比如在前面我们刚才提到了今天大模型的训练,它不是针对一个特定站,他要同时处理海量任务,他把这些海量任务全部都改成这种生成next token,prediction的这样的一个事情,那么所以既然你同时处理海量任务,那你研究的所谓的泛化,已经和过去单任务的泛化,这本身就已经完全的不一样,这是第一点,第二点呢是刚才所说的啊,经典的机器学习里面,全部都是假定独立同分布啊,我的训练数据和测试数据是独立同分布的,那么在大模型时代,这个事情又完全变调,根本就没有独立同分布这样的一个假设,因为你在实际使用的时候,我们通常叫做inference啊,用influence这个词就是你在做influence的时候,甚至你的任务都跟你训练里面,训练集里面的任务是不一样,你可能都是一个新的任务,那么和谈这种独立同分布完全没有这样的假设,并OK所以今天从泛化的角度,我们也面临着全新的问题,所以总结一下在今天大模型时代,过去40年,这个机器学习理论啊,建立的一些很经典的一些结果,当然甚至也包含优化等等,这些领域里面建立的一些经典的机器学习理论,现在面临着全新的问题,那么在这些新问题下,一些旧的理论可能都已经不适用了,你必须要去建立对新问题有意义的,直面这些新问题的理论,所以呢这是对今天的啊,这个大模型理论提出了一个非常大的挑战,那么也是巨大的机会啊,咳好,那么今天呢我在这个呃这部分呢,主要是和大家呢一起来探讨一下呃,china fault啊,思维链这样的一个事情啊,那么咳我先呢和大家一起来看一看,思维链是什么啊,然后我们再一起来分析思维链,为什么对于大模型的推理能力有巨大的帮助啊,这样的一个呃理论嗯好,那么关于思维链呢,如果讨论思维链,其实我们得回顾一下,就是大模型刚刚出现的时代,他是什么样的,那么大模型的第一个能给大家广泛使用的版本,就是chat gbt,其实这个事情呢已经过去了两年半了嗯,在2022年大概11月份的时候,OpenAI呢这个放出来第一个版本ChatGPT,如果有各位在那个时候用过大模型的话,其实会发现那个时候大模型对于一些推理问题,他是非常弱的,什么样的推理问题呢,有些特别简单的推理问题,比如说大家看我左上角这个例子,这个例子其实本质上就是算一个加减乘除,就是一个四则运算的这么一个简单的问题,但是你在那个时代,如果用chat GDP的版本,你问他这么一个加减乘除的问题,他就会直接给你输出一个答案,而这个答案呢往往都是错误的,就这么简单的事,如果大家还有印象,有印象的话,其实在ChatGPT刚出来的时候,有一些人会编一些小学水平的应用题,比如说鸡兔同笼问题啊,这种问题呢当然你如果会解线性方程,那那当然这就是一个吹牛的事了,咳但是当时的大模型根本做不了这样的问题,既然当时的大模型,你一这个会直接输出答案,那么是完全这个大部,绝大部分情况下是完全错误,所以这就是当时大模型面临的一个最大的困难,就是对于一些很简单的数学问题,这么加减乘除的问题,甚至一些就是解这种线性方程组的,这么简单的问题,它都不对啊,看左左这个左下角啊也是一样的,这么一个问题都不对,那怎么办呢,后来过了一小段时间,非常偶然的,有一些研究人员,他们是纯粹从实验里去试尝试啊,发现啊对大模型我做一个很简单的一个事儿,就能把大模型推理能力提高,那这个很简单的事,就是所谓的china f,那具体他怎么做呢,他就是在当时啊,他尝试说我给这个大模型我的prompt啊,这prompt就是提示词嘛,就是你跟大模型对话的时候,其实你要跟他说一些话,你相当于要提示他啊,就是你跟大毛行说的时候啊,不要直接就问这个问题,你呢跟他说一下,你跟他说一下,Let's think step by step,我们一步一步的来做,就是你提示这个大模型,要把步骤嗯也要给我输出出来,那么就用了这么简单的一个trick咳,那么后来发现大模型啊,只要他把这个中间过程步骤也都输出出来,那么他这个正确率就大幅度的提高了,所以这是第一种啊,这个就叫做chain of f,就是把这个步骤一步一步的也都输出出来,这就是chain f,那么刚才这种方式其实叫做zero shot chain f,那还有一类呢叫做few shots,China fa,Few shots,什么意思,就是我我不是说给他一个提示词,让他think step by step,而是要给他一个例子,我给他一个例子,这个例子里面是一步一步的去思维,把这个思维过程一步一步的展示出来,那么大模型看到这个例子之后,他在回答问题的时候就会认模仿这个例子去做,那么也是可以一步一步的把这个做对,所以呢这就是所谓china fund,就是思维链核心,是说不是直接一步输出结果,而是要把这个思维的过程,一步一步的也都疏通出来啊,要增加这个思维的过程,这就叫做思维列啊,这儿呢就是举一个很简单的一个例子,这这就是刚才说的解线性方程的例子,如果它直接输出呢,基本上嗯再简单的方程也是错的,但是呢如果让它输出过程,那基本上都是对的,当然到了今天,思维链已经成了这个达模型里面,必不可少的一个环节,所以今天大家看到的全部都是有思维过程,有思维链的啊,这样的结论结果啊,咳咳嗯好,那么既然我们今天讨论的是理论呢,所以核心的想法是,我们希望对思维链有一个基本的理解,就是从理论上能够对思维链有一些inside,当然最终目的呢是希望我们的理论的理解,能为将来去设计更好的方法提供一些帮助啊,提供一些指导,所以呢我们嗯希望从两个方面去理解,思维链是怎么回事,这两个方面都是在今天我前面讲了,这个理论研究大模型其实有啊三个方面,就有三个类别可以去研究,分别是表达能力优化和泛化啊,那么我们讨论思维链呢,其实都是从这个表达能力这个角度去讨论,那么刚才也谈到了,表达能力在大模型里,绝不是过去简单的逼近论啊,这么样的一个一个一个事情,因为今天我们不仅涉及到模型,也就是函数类本身,我们还涉及到了它的推理过程,思维链这样的推理过程,所以它要变得复杂得多啊,而且呢后面我们会看到,我们不是像经典的数学那样,只是谈个逼近啊,我们今天呢还要谈非常重要的一个,效率的问题啊,咳好,那么具体来说我们思维链看哪两个方面呢,我们来看第一如果不用思维链的话,就今天的大模型,如果不用思维链,他是直接去回答问题,那么他的能力上限,有没有可能从理论上给出明确的刻画啊,这是第一部分,那么第二部分是如果我们允许大模型用词为列,那么此时这个大模型用了思维链的大模型,他的能力又是一个什么样的水平啊,这样的两个问题,当然我刚才在描述这两个问题的时候,其实还不是一种严格的描述啊,因为这里面要牵扯到你具体解决的是什么问题,和你对这个大模型用和不用思维链,他的能力是用一种什么方式刻画,既然要通过理论研究,后面我们必须要把这两个事,是用一种理论的方式非常明确的得得表达出来,得定义好好,那么先说说第一个,就是,我们要讨论大模型用和不用思维链能力的差别,我们得针对一些问题,那么在这里呢为了清楚地进行理论研究,我们就对一些数学和推理问题来进行研究,我们特别就针对少量的啊,非常简单的一些数学问题,我们就对两类,比如说第一类就是简单的四则运算,就是加减乘除四则运算问题,我们来看用和不用思维链大模型,分别是什么样的能力,我们还看比如说就是解线性方程组,这么简单的问题,用和不用思位列啊,他的能力分别如何,Ok,好那么为了要做这样的研究,我们必须呢要把一些细节也定义清楚,什么细节呢,就刚才说了,今天我们讨论思维链,实际上是讨论大模型所用的深度神经网络,和思维链这样的推理过程,二者的合在一起来讨论,那么今天的大模型,今天几乎所有的大模型所用的结构,都是叫做transformer这样的结构啊,我我不知道这个呃吉他呃,几门课程对transformer这样的结构啊,啊有没有一个非常详细的介绍啊,咱们今天下午应该课程内容里会给大家,把这个从former相对来讲,作为一个较为详细深入的介绍啊,那transformer,实际上它是有自己非常强的特点啊,在今天从性能上来讲,它是最好的,这个深度神经网络模型,那么我在这里想说的呢是说,我们要对一个带思维利安过程的,transformer神经网络进行探讨的话,后面我们要注意一个事情,就是你在计算机中,永远用到的是有限精度的计算,你所有的数都只表示成有效,有效数字只能表示为有限内,那么如果用二进制的话,你的这个位数就是比特数啊,嗯永远不可能去精确地表示一个实数,因为如果你的计算机能够精确表示实数,并且还能进行运算的话,那么今天这个理论计算机里一些最根本的问题,也可以认为是数学里的一些根本问题,那结论就会发生变化,比如说所谓的这个可计算性的一些问题,都会发生变化啊,所以在这呢先要把这个明确,就是我们用多少有效数字啊,这里面我们用一个所谓叫做log n比特的精度,这个N指的是我transformer输入的长度是N,那么我的精度啊,用这个有效数字这个事情本身也是有意义的,因为在今天其实对大模型的应用,特别是现在大家有时候,手机上也布一些大模型啊,今天当然还必须通过联网的方式,这个在云端去去做,但是实际上在如果将来是布在这种端侧,像手机这样的端侧,大家可能会希望用非常少的有效数字啊,呸就非常低的精度能够进行运算,那么一个很自然的问题是,这种低精度的运算到底会不会影响你的性能,其实大家今天可以,如果对一些这个技术细节了解的话,deep sick的模型它非常重要的一点,就是他可以用这个八比特来进行运算,它比其他人用32比特甚至64比特运算,这个精度要低得多得多,但是呢它能够达到这个完全一样,几乎完全一样的这种性质啊,好,K那么刚才谈了一个精度的问题啊,那么接下来我们就要说最核心的一些内容了啊,那么这一页呢,实际上是我们对transformer,这样的深度神经网络模型的一个真正的认识,就是深度神经网络,可能大家应该也有一个基本的了解了啊,对transformer可能也有一个很大概的了解,那么这一页我们的inside是什么呢,就是今天的transformer这样的神经网络,实际上和上个世纪在理论计算机领域里面,所研究的一类模型叫做circuit,也就是电路有着极其深的联系,有非常深刻的联系,甚至我们都可以说今天的transformer,就是某一类circuit,那么是因为是,首先我们来看一看什么是circuit,什么是电路,这个电路在上世纪,为什么会在理论计算机里面去研究呢,是因为理论计算机里面有一个排名第一,重要的理论问题,就是所谓PNP问题,我估计在座的各位,其中可能有一部分也听说过PNP问题啊,那么什么是PNP问题,他讲的是一个计算复杂度的事情,就是说P代表我这个问题是存在着一个算法,可以在多项式时间之内解决,这就叫做P问题,那么NP问题呢用今天的语言来讲,是我不知道是不是存在着一个算法,能在多项式时间之内解决,但是我知道如果你给我一个这个问题的答案,我一定存在着一个算法,可以在多项式时间之内验证,这个答案是正确的还是错误的,这就叫做NP,可以想象NP啊,直觉上的NP一定是比P更困难的问题啊,那么显然P这个问题,这类问题呢是包含在NP这这一类问题里面,但是理论计算机就想问,P和NP这两个关于问题的集合到底相不相等,它是一个真包含关系,还是一个相等的一个关系啊,这个问题看上去答案似乎显然是P不等于原P,但是呢已经时间过了差不多55年了,到今天可以说关于这个问题呢,还大家毫无眉目,不知道怎么去证明好,那么回到这个thirty,这个是上世纪大概70年代末到80年代,当时大家认为非常有希望,能够解决PNP问题的一个思路,他研究的是什么呢,它就是研究的这种所谓叫做circuit,把这个电路看作一种啊啊解决问题的模型,他去研究我要解决一些模型,那需要用多么复杂的电路,那什么是电路呢,其实电路在那个时候的研究,就是一些所谓叫做逻辑电路,这个逻辑电路呢由一些门和这种连线组成,那么门就是gate,它通常最基本的gate就是这种与或非,由这种与或非组成,其实就是这种既最简单的布尔逻辑,OK嗯那么你有一些输入,有一些这种啊玻璃这种二呃,二进制的这样的一一种输入,那么你通过这种逻与或非逻辑运算,最后可以有一个输出啊,OK那么关于circuit,大家就问的是啊,那么我给你一个函数,你需要用多么复杂的电路,才能把这个函数给实现,那么这就叫做所谓电路复杂度,好,那么我们这个关于今天大模型,transformer最核心的inside是什么呢,我们想说的是今天你的transformer,大家看这这右面的图就是transformer的结构啊,具体的可以在今天下午啊给给大家介绍啊,嗯虽然看起来很复杂,但是它本质上其实就是一种电路,而且跟过去所研究的某一类复杂度的电路,叫做TC0,我待会儿解释什么是TC0的电路,可以说甚至没有区别,这个区别是完全可以忽略不计的啊,所以这个也我觉得也容易理解,因为今天你的transformer,也是由一些所谓的神经元和这种神经元之间的,连接权重所表示的那么区别和经典的电路,就在于今天的神经元和之前的gate,可能有一些计算的不同啊,这个连线呢那么过去可能做的是这种,有可能这个布尔问题,今天呢可能是做的是这种实数的一些操作啊,就这样的一些小差别,但这些不是本质啊,好咳咳,那么TC0刚才说了是一类这个呃电路复杂度,我稍微解释一下什么是TC0啊,TC0什么意思,TC首先这先不看这个上面的角标,所谓TC想想说的是,我这里面的门用的是一类比较特殊的gate,叫做fresh hogate,什么是THRESHHOLD,就是它的比如说这个输入经过一个运算啊,如果大于零,我就让你输出一啊,如果小于零,这个那我就让他比如说输入零,就是一个threshold function啊,这就叫做threshold gain,那么上面的这个零什么含义呢,代表了我这个电路的深度,它是一个constant,它是一个不依赖于我的输入的,大写输入的规模啊,那么如果这个上面的角标是一,就代表我的这个电路的深度,和我的输入规模的关系是一个对数关系啊,好咳,那么TC0刚才说了,就是这样的一类电路,首先它的门是用threshold gate,然后这个上面零代表我电路的深度,是一个constant深度,OK那么我们最核心的这个结论如果用一种啊,这个不是很严格的非正式的一种表示,想说的就是当你只用大模型,只用transformer啊,现在没有用channel f的时候,那你这个transformer这个神经网络,他的表达能力就和TC0这一类电路的表达能力,是完全一样的,是相等的关系,但是今天你要做的大量的推理的任务,你要做的大量的这种planning的任务,我们后面会看到,即使是像四则运算这么简单的问题,像解一个线性方程组这样的问题,它都不是TC0这类电路所可以解决的,哎这就是最核心的,所以本质上来讲,就是当你只用transformer大模型,你只用这个模型本身只用transformer,他的表达能力是PV0这个类,但是这些你要处理的问题,它需要的电路复杂度是要远远高于TV0这一类,这是一个最最基本的这样的一个informal的结论,Ok,好,那么这个我们来来看一看,为什么刚才说,四则运算和解线性方程它这么简单的问题,他为什么不能被这个TC0这类解决呢,我在这和大家,这是第二个最重要的insight,为什么四则运算这么简单的问题,解线性方程组的问题,它不能用一个很简单的电路来解决啊,不能用一个很简单的transformer来解决,那它核心的原因是什么,核心的原因就是PC0,这类电路,其实主要描述的是一种并行计算的能力,那么刚才说了,Transformer,transformer这样的啊结构,它本质上就等于TC0这类啊,电路复杂度,大家可以想象一下,transformer也好,或者其他我们的深度神经网络也好,它实际上对输入来讲就是一种并行的计算啊,这个呃,呃,今天下午呢,可能会给大家再去介绍这个transformer啊,这个大模型就是今天其实所有的大模型本身,它对输入是没有顺序关系的,它是一种纯粹的并行计算,就是你给他,比如说输入一句话,其实哪个词是第一个词,哪个词是第二个词,大模型是没有区别的,那么今天为了要解决这个问题,必须用要用一种叫做position encoding的方式去解决,也就是我要增加一些信息,把哪个词是第一个词,加入这样的一个叫做位置编码,Position coding,再输入伟达模型,这样大模型才能知道哪个词,它是处在哪个位置,所以也就是说模型从计算的角度来讲,它是一种纯并行的,它根本无法区分输入谁在前谁在后,今天完全是由于我们对每一个输入,增加了它的位置信息,大模型才知道谁在前谁在后的,因此今天的大模型本质上,它是在做一种并行计算,就它本身完全是在做并行计算,OK因此呢这就是嗯回到我们刚才的问题,由正是因为你达模型是在做并行计算,那他的能力就是被并行计算的能力所刻画好,那么回到我们刚才这个问题,就是说四则运算也好啊,这个线性方程也好,这么简单的问题,他为什么不能被大模型,为什么不能被TC0这类电路所解决,核心原因就在于四则运算和解线性方程,它是一个高度串行而不可并行的问题,就是它如果你只看它的串行计算,它是非常简单的,它太简单了,但是如果你要想用并行计算方式,去解决这两个问题,它就变成了极困难问题什么,即所谓极困难,就是他所需要的complexity,就变得非常非常的大啊,大家这个其实呢稍微想一想就很容易理解啊,就哪怕是负责运算运算,大家可以看到它是本质上不可并行的一种方式,它必须用一种串行的方式来解决啊,嗯因此这就是今天,为什么transformer这样的模型,你如果只利用大模型本身,不用其他的推理,用思维链这样的工具,连四则运算啊和解线性方程都做不好,当然这里面我们虽然是用四轮运算,线性方程组为例,其实对大量其他的推理问题,概率问题,它里面都含有非常多线性这种串性的成分,而是一种非并性的成分,因此呢这也是今天大模型本身,他做推理是他的一个这个弱点短板好,这是前面的结论,那么我们接下来来看一看,如果我允许用china fat这种方式大模型会怎么样,那我们的结论就是你大模型用china f,刚才已经谈过了,什么是china f,就是我允许这个模型step by step,它把过程也全部都输入输出出来,那么transformer加chain of thought,为什么变强了,因为我们前面看到了啊,就是这个今天这个transformer,它用的是一种自回归的方式,也就是它的输入给了他这个部分输入之后,这个模型啊有一个输出,它会把输出再放回到输入里面,不断的反复循环使用这样的一个成果,那你用了channel thought,让他把过程也一步一步输出出来,而不是直接一步给答案,那么它反复利用transformer的次数,就大大的增加,你某种意义上来讲,可以认为是用了一个大的多的模型,在做这样的事情,对于我们最主要的一个理论,结果是说你transformer加上channel fat之后,他的表达能力就远远的超过了,PC0这类电路啊,因此他可以去解决一些刚才所说的,从并行计算的意义上来讲,非常困难的啊,这样的问题咳好,那么transformer加FA channel flat,他的表达能力究竟到一个什么水平呢,啊从并行计算,那我们可以证明它达到一个叫做p complete,这样的水平,那么什么叫p complete,p complete就是在所有这个P问题,刚才说了这个panami time问题里,从并行的角度来讲,最困难的问题它都可以解决啊,那么也就是说,那今天大家通常都认为P就是panomeo time,是大家可以去解决,如果不是跑了monial time也能解决,那就是非常难的问题啊,咳所以这个结论想说的是,transformer加上china f,他其实可以把pomi time,P问题里面所有的问题都能够以这种给解决掉,好咳咳咳好,那么以上呢就是啊关于这个啊,transformer用chain for哦,不用chain f啊,总结一下,就是前面我们得到了一个正面的结论,和一个负面的结论,正面的结论就是transformer加china p,他的表达能力,它能解决问题的能力是极强的,它可以到p complete,那么如果你只是用transformer,不用chain f,那实际上它就是一个TC0,是一个非常弱的这样的一个一一个类比啊,咳OK这是啊啊这个以我们主要的结论,当然我们限于时间关系,只能给大家用这样的一种,informal的方式去介绍介绍,但后面呢我会把这个相关的一些论文文献啊,这些材料呢这个后面也发给大家,就如果大家感兴趣的话,可以去看后面这个论文里面非常技术性,这个具体内容,OK嗯好,那么最后呢我再嗯提几个吧,这个应该说更前沿的这个研究,那今天呢除了transformer以外,其实大家还关心就是你一个问题,是我有没有更好的这种模型结构,我能不能有一天啊,比transformer更好的这个结构,所以大家可能也许听到过有一些新的一些结构,比如叫MANA这样的结构,那他的出发点是什么,为什么我们希望设计一些新的啊,比transformer更好的结构呢,因为transformer他从计算的角度来讲,它有一个问题是,由于transformer用attention这样的,具体的一个模块,所以呢它的复杂度是一个N平方,也就是它每一个成分都要跟其他所有的成分啊,去做attention,所以这就是一个N平方的这样的一个复杂度,那么从计算的角度来讲,这个效率比较低,那么人们希望呢,能够用一些更efficient的transformer,比如说MANA这样的一些linear的法律,所以呢还可以去探讨,像MANA或者其他的一些efficient transformer啊,我他的表达能力,是不是能够和transformer一样好啊,比如说呢这个是也一个一个有有一定,就我们一个其中的一个工作,就是用MANA这样的新的transformer的结构,他的表达能力是跟经典的transformer一样,还是说他其实明显下降啊,那么这个回答呢是在worst case情况下,他的表达能力是下降的,但是如果限制在某些具体问题上,某些类别的具体问题上,那么他们的表达能力是一样的,所以这就取决于你要解决啊什么样的问题啊,好那么我想关于channel f呢,我给大家的介绍啊,应该是也是一个比较high level的一个介绍,就到这里,相关的论文其实内容是非常之多的啊,我后面会给大家啊也都整理出来啊,分享给大家,那么接下来呢就请这个单子康博士给大家介绍,这个关于revenue,我我先这个在前面给大家整体的做一个介绍,就是今天特别是随着deep seek这个R1的版本,我不知道大家有没有用过啊,就是deep seek这个当然现在又推出了更新的版本,那这样的版本就是完全是为了啊,有更强大的推理能力,那么今天为了要有这种更强大的推理能力,其实核心是要用reinforce learning,就是强化学习呃,我看了一下,好像在我们这个五天的课程里,其实没有呃,专门讲强化学习的部分,所以呢就接下来呢,可能首先花一部分时间给大家介绍一下这个啊,强化学习本身啊,大家可能前面也看到了,就在上个月呃,这个今年的图灵奖,也就是计算机领域里面的最高奖,就相当于计算机界的这个嗯啊诺贝尔奖吧,图灵奖也是颁给了两位强化学习的先驱学者啊,rich sen和他导师人朱巴黎,所以呢你这个强化学习,在今天的机器学习领域里面,在人工智能里面是占有这个举足轻重的地位,特别是对于今天的这个reasoning啊,这强化学习是极重要的,基本上大家做最好都是用强化学习的方式啊,那么强化学习其实它的根源是来自,上世纪五五十年代控制论里面,如果大家有有所了解,就是这个叫做贝尔曼方程,就是贝尔曼当时这个做控制,提出这个叫做贝尔曼方程,这个应该是很有名的啊,那么强化学习跟之前这个贝尔曼,他做的工作的区别在哪里,就贝尔曼的工作,他提出贝尔曼方程,他其实想解决的是这个所谓叫做dynamic programming,对动态规划的问题,那么在那个时代,贝尔曼他们等人的工作去研究这个啊,动态规划是假定我的系统的参数,已经都完全知道,我是在知道系统参数的条件下,想办法如何去解决这个planning一个规划的问题,但是强化学习跟这个刚才说的namic programming,最大的区别是在于是我不知道,我假设我根本不知道我这个系统参数啊,就待会大家可以看到这个所谓系统,就是一个这个所谓叫做马尔可夫决策过程,马尔可夫decision process啊,它里面的一些这个转移参数和reward等等,那么当你不知道这些系统的参数的时候,你必须用一种学习的方式去学相关的信息,这就是强化学习和经典的这个贝尔曼,他们的这个delement programming工作最大的差异,那么在今天,其实大家应该已因为已经到了2025年了,大家知道其实你是没有可能知道系统参数的,就这件事情几乎是不可能做到的,你必须去去学习啊,好那么这是关于强化学习,那么强化学习其实呢这个经过几十年,也差不多有将近40年的时间啊,也建立了嗯这一定的理论体系,那么在这个理论体系和算法的基础之上,那么今天被用到了这个大模型的wave啊,所以接下来呢就是分成两个部分啊,一部分是给大家先介绍一下强化学习本身,然后呢是看强化学习在大模型reasoning里面,它是有几种方式,具体是都是如何应用好,那那就请这个啊子康来给大家继续,嗯好的嗯现,好的谢谢王老师之前的分享,然后呃呃各位老师同学大家好,我是山子康,我是王老师的学生,然后今天呃我将跟大家讨论一下,强化学习这个这一门呃这门技术,以及这门技术在大语言模型中的应用,然后特别是对于增强大语言模型,推理能力的一些尝试,然后呃呃我我今天的报告也将分为两部分呃,首先是对于强学习这个路径做一呃,做一个比相对简略,但是又呃包含一些关键信息的一些介绍,然后呃呃我们将以这个介绍为基础,来看一下这个整个强学习基础,是怎么在大语言模型当中应用的,对首先是一个最基本的概览,就是当我们说强化学习时,我们指的是指的是什么,它放到整个呃人工智能中的框架的话,大概是如左边这张图所示,就是最外层是整个人工智能学科,我们希望用人工的方式建立呃所谓的智能,而今天的人工智能的主流是机器学习,也就是说我们希望用学习类算法使用呃,使用机器和某种学习类算法,让它自动学习出一些,我们希望他学习到的一些表征,然后机器学习内部又呃具体的分为很多个流派,然后其中比较典型的是监督学习,这个也也也也算是现代经济学习当中,占据相对主流的一个一一个范式,然后与监督学习相对应的,还有今天我们的的主主角就是强化学习,他是另外一种学习流派,我们随后会分析一下这些流派之间的区别,然后大家可能还听说过一个词叫深度学习,深度学习的这个概念,它更多的是由于大概10年到现在为止,深度神经网络这个呃这个模型结构的发展,而导致了现在有一个名词叫深度学习,它并不是一个学习范式上的一个概念,所以它呃与之相对的,它跟监督学习和强化学习的关系也并不是,并不是说呃完全独立,而是深度学习可以用于深度学习,深度学习也可以用于强化学习,然后它们之间都是一些交叉的关系,所以我我们今当我们今天提到强化学习,我们其实很多时候指的是强化学习,当中使用了深度学习的技术,也都是所谓的深度强化学习,然后呃刚所说这些名词的的的大概的关系是,如左边这张图所示,然后接下来是呃所谓强化学习,它是一种学习范式,当我指学习范式时,指的是跟其他的一些学习方式相对,然后最基本的监督学习,我们是希望从一些所谓带标注的数据中学习,一些学习出一个predictor,它的作用是避免一个input,它能够预测出一个呃呃meaningful的结果,一个一个典型的例子就是呃就是图样图像识别,图像识别的时候,我我我们是用经济学习的方式,可以学习学习一个呃图像分类器,这样他在之后的任何一张图片的输入上,就能够预测自己对于这个图片类别的理解,像这样一样呃,训练一个所谓的predictor,这样用的学习方式叫监督学习,然后另外有一类叫叫呃unsurprise learning,它是当我们没有标注数据集的时候,我们可以做一些尝试,主要是来去学习学习一些数据内部的一些表征,而我们今天讲强化学习,他的做法其实是我们需要去学一个叫decision policy,它能呃,呃我们通过一些车管ARL来去学习一个design policy,然后这这个事情我我具体举一个例子,这个例子也经常被大家来使用,就是当你去希望去呃,呃希望训练一条宠物狗的时候,就是呃呃宠物狗作为一个智能体,它是一个比较长期存在的,长期存在的一个智能体,就是它并不是像精度学习一样,呃given一个输入,它吐出一个输出,然后整个过程就结束了,而是我们希望这个宠物狗它在日常生活中,在一个相对长的一个序列,序序列的决策过程中,能够一不断的提供决策,那我们对于这样的一个呃,这样的一个系统的一个训练的方式,也是我们需要根据他的一些行为,来给他一定的反馈,比如说呃宠宠呃,呃这个这个宠物狗,它可能会做出一些比较好的行为,比如说当你丢出一个树枝,他过去把它捡了并捡回来了,那你就会认为他目前做的这个行为是比较好的,你可能会额拍拍他的脑袋,然后给给他一点好吃的,就是给他一些正向的一些反馈,但另一方面就是如果你发现呃,发现你的狗呃不是很乖,然后到处乱咬,但这个时候你可能要给他一些惩罚,比如说让他少吃一顿饭,总之就是我们是通过把宠物狗放放,放在一个环境中,让他不断的与这个所所谓的环境,也都是你也就是你对他的这个和监督过程,你把它放在一个环境中,让它进行一些训练,然后根据他的表现来给他一些,正向的或者负面的奖励,然后呃由于狗本身有一定的智能,所以它就能够从你给他这个奖励中,学习出自己应该做什么,不自己不应该做什么,最终形成一个有一定能力的,一个能够持续决策的一个智能题,所以呃强化学习,我们从直观上说,就是我们希望复刻宠物狗自己的一个学习过程,我们希希望能够通过不断的试错,然后以及获得奖励,我们希望通过这样一个过程,能够学学习一个学习,怎么面对一个复杂情形的时候,应该如何去决策,呃呃我刚说这个过程,我们之后会从数学上去比较正式的去描述它,然后呃呃最后我我简单介绍一下强化学习,这个领域,在历史上的一些比较成功的一些一些地方,就是呃这个大家其实也可能听到的,最左边这个是强化学习,在在attery video games上达到了接近接近人类水平,就是在使用强化学习额,在一些电子游戏上能够训练出非常强的智能体,它它能够跟人类的先进的呃,先进的玩家达到接近的水平,然后这个过程使用的技术叫DQN,我们今天也会也会提提到嗯,然后第第二个是ALPHAGO,这个大家应该也清楚,就是使用强化学习的方法训练了一个呃,训练了一个围棋的AI,然后这个围棋的AI,成功的击败了人类的围棋冠军,然后呃阿发go背后的技术也是强化学习,然后他大他用的大概基础叫MCTS,这个我们今天问呃,这个由于比较复杂,我们今天今天不会讲,然后最后就是在单元模型上的应用,然后比比如说呃呃最早的open axd bt,然后呃后来OpenAI的O1推理大模型一,那直到今天的这个deep sr r1,就是在这些大语言模型的呃的进步的背后,其实都是有很多强化学习的技术,再去做一些支持的,这个我们最后也会详细去探讨,然后呃,接下来就是我们要讲一下强化学习的一些,数学的一些基础,就是我刚刚举的那个训练宠物的这个例子,是在直观上强化学习在做的事情,但是我们现在要尝试把这个事情,去给正式的形式化一下,然后正式的形式化也就能够带来一些呃,是附属的分析,从而引成出一些强学习算法,就是这里先,我们把这个环境形式化的一个主要的工具,是叫马尔可夫决策过程,就是MDP,然后这个决策过程,它会包含以下的这样的一些参数,首先是这个S它是它是这个状态空间,然后A是行行为空间,然后在这个基础上,还有一些有一定义的一些函数,第一个函数是是P,它是transition dynamic function,就是given一个S,given一个A,然后他来决定在S这个state上执行action a之后,会进入接下来进入一个什么样的state,然后它它可能是随机的,所以这样这是一个额,这是一个在SA之上的一的一个概率函数,然后是RR是一个roll的function,就是skit s在git a的情况下,环境会额会被智能体赋予一个reword,这个reword可正可负来描述描述环境,环境对你的这个行为的一个反馈,然后最后就是关于初始state,六零会有这样一个初始分布,然后我我们希望我们比较关心的是,所谓的一个policy policy,就是在呃even s的情况下,policy需要需要理decide,我应该执行执行哪个action,然后这个PY1同时也可以是随机的,所以它是定义在RS呃,第号基A到R的这样的一个方程上,然后我们有了一个policy,有了一个FMDP之后,我们会在policy在环境当中去执行这个policy,来得到一段经验,然后这段经验也被称为高out,它是包含从初始state s0出发,然后policy呃,policy可以会决定A0,环境给R0,然后同时环境进入下一个时刻S1,我刚才说的比较抽象,我们可以结合下面这个例子来看一下,就是呃整个把policy放在环境,放在m DP environment当中执行,然后或者experience的过程是如下面这张图所示,首先这个环境会由某一个初始分布去三某处,第一个state s0,然后呃我们在我们把这个S0给policy policy,它会定位S0,它会决策,然后给出这个时候我们应该执行哪个action,然后这个action也就是A0,然后额,当这个环境发现你在S0中执行了A0之后,首先他会给你一个立体的一个reward r0,然后其次这个环境会transit到下一个state as s1,这样就就完成了一次执行action的过程,然后然后在S1上,我们依然是让policy再次decide,要要进行一个什么样的action,然后会获得reword,然后然后会继续传,继续transit,这个事情就会一直执行下去,所以呃我们也可以说这是一个序列决策过程,我们希望去学习到的PY派,它是一个这一个decision policy,它能够对于呃hopefully,对于任何一个S他能够给出一个比较mini或者A,然后额额这里也简简单的说一下,就是在我们这个模型中,这个MMDP它会一直的永远的执行下去,这是为了方便理论分析来做的,在实际过程中,这个执行会我们会预先设置一个阈值T,这个T也叫horizon,当执行超过替补之后,这个执行就会就会截止,然后所以我们也可以看到这整个formulation,它确实是符合马可夫性质的,就是嗯原则上的,当我们执行的到了某一步的时候,我们所有的过程,比如说这个policy去DEDECIDE应该要要什么action,然后以及这个reword或者transition测过程,都只是呃depend on当前的state s,而不depend on所有的hihistory state,然后呃上面说白了formulation,我们接下来看这个整个L的过程,我们希望希望L做个什么事情,就是我们希望LL能够学到一个policy,然后呃但是这个policy是能够最大化累积的,累积的呃,discounted reward就是它的一个一个形式化,是由上面这个公式所刻画半点,首先是一个期望,这个期望是对于这个policy能够执行的,roll out来进行的,然后期望内部是这一个robot能够获得的,累积的奖励,因为是累积,所以它是从T等于零时刻一直sum到到无穷,这是包含了这个rot内的每个时刻,然后在这个每个每个时刻中,这个root都可以从环境获得一获得一个这个word r,所以他是some of r,而这个啊啊前面还加了一个伽马,伽马是一个0~1之间的数,一般比接近一,比如设为设为设为0.99,他是一个discut factor,加入伽马的作用,也是说就是一方面这个我我们认为比较近,比较近的未来能够获得的reword,会比比较远的未来能够获得的如word,就是是要更加重要的,然后这个事情也在一些其他例子,比如说一些呃金融上的例子,额银行,比如说钱存到银行里,或酒或者利息会跟这个事情相关,当然在数学上引入伽马,也可以保证这个一直想不到无穷的这个东西,能够一般来说它的收敛性会更好,然后呃呃简单来说就是go of all l,我们希望做的事能学习这样的一个policy派,它能够它在整个函数列表当中,能够maximize整个过程的累积的word期望,然后呃刚刚讲的这些内容,全是RL的一些抽象formulation,我们讲了讲MDP,我们然后我们讲了policy以及POY结构,那么接下来具体看几个MDP的例子,这样也也方便大家呃理解啊,要具体在干一个什么事情,我们举的第一个例子就是棋盘呃,就是围棋,这这是在阿法go当中使用的MDP,那它它里这里运用的状态空间,S就是所有的所有的这个合法的C盘,例如嗯我们知道棋盘是棋盘,是19×19,然后然后每一个点可能是黑棋,白棋或者空,所以这个S就是三的1919次幂,这么大的一个空间,然后呃ACTRESPACE就是我们能够执行action的地方,那在围棋也就是说呃1919的每一个int session,我们都都是潜在的,可能这些action的,然后这里的这个transition p也很简单,当我们决定执行一个action之后,我们就把这个棋子放下去,所以这个分transition p它是呃,它是没有任何随机性的,就是我们把这个A去放在S上面,这里的这个real signal它还是一个非常稀疏的signal,他是会在整个周二的结束,也就是你整局为棋下完之后,根据你赢还是不赢,给你个过的,你要是赢了我,我就我就给你个正手过的,比如说给你一,你要是输了就给你零,然后这一个single reword是在呃terminate,terminate之后的那个terminate的state上,会给你这样一个系数的word,对于所有其他的state,这个如果都是零,然后初初始初始state的分布,就是简单的是一个一个m p board,这是MDP的第一个例子,然后呃,第二个例子就是关于这个ATTERY,video game的一个例子,这个例子也是在那个DQN当中使用MVP呃,我我们可以看到呃,在电子游戏当中,这个S嗯,嗯所谓的一个状态,其实是这个游戏内部的一个逻辑,呃,这个内部的逻辑我们其实是看不到的,我们能够看到的是这个游戏的画面,这个画面称为称为observation,它是完全由内部的logic来决定的,所以呃这里的这个steam space,其实是那个所有可能的内部逻辑,然后这里的这个MVP就是也做了一些修改,叫partially observe observable MVP,也也就是在MTP的基础上,我们不能我们观察不到S,我们只能观察到有S引发的,一的一一个observation,而这里的这个这个observation就是整个screen,然后他的A呃呃呃,action就是玩家能够使用键盘进行一些输入,比如说WASD上下左右,然后transition p就是游戏逻辑的更新呃,呃奖励R就是这个游戏内部的一些,分数上的一些变化,而RO0就是这个初始初始游戏逻辑的一个,可能的分布,最后的一个例子就是我们今天的大模型当中的,这个MVP,这个MDP,也也呃他这里的state space,是所所有可能可能的一个partial sequence,所以PARROSEQUENCE,就是包含用户输入的这个提示词,以及这个模型在在auto progressive,生成response的过程中,生存到某一步停下的一个pole sequence,比如说我左边举的这个例子,就是这个用户的提示词是为我写一句,写一句绝句,然后这个炮车整个生成过程,它是会会逐个生成的对吧,然后我们如果截止到生成过程中的某一步,我们就会发现嗯,截止到这步之前的这个part of response,它和前面的这个BRT,构成了一个的一个power sequence,这个power sequence,就是呃大大模型的MDP当中的S,然后它的类space就是所有可能的这样的S,然后他的action space是是这个大模型的字字典,也也就是它可能输出的所有字符的一个集合,他的这个transition也是很简单,就是我们目前有个power sequence,我们目前有一个呃所谓的next token,然后我们做的事情就是把next token,再去去append到这个power sequence上,得到下一个power sequence,这就是这个MDP的这个空间的过程,然后呃这个过程的reward,我们之后讲大模型的时候,会专门去提及这个REWOR,一般是基用基于模型或者基于规则的一个FEBACK,最后这个rolling就是初始用户,用户prompt的一个分布,下面是三个关于MDP的例子,然后我们现在应该已经比较熟悉MDMDP,具体在干一个什么事情,然后我我们接接下来简单讲一下RL过程中,就是我们学习这样一个policy的,一个主要难点在哪里,它大大概是有三个三个难点,第一个难点就是整个呃state和action,这个space是非常非常大的,就这个space大和呃,一般我们说深度学习尝试解决的这个,high dimensional data性质还不是一样,因为它这个这样的一个state,每一个state都是一个hdmal data,而这样一个space会包含,一般来说是指数级这么大的一个space,这个space是非常非常大,在这样巨大一个space上,我们就会就会面临一个很经典的一个问题,就是我们要在探索还是利用,我们要进一步再去多尝试一下,我们之前没有探索过state,看看这些没探索过state上的结果,会不会比现有的更好,还是我们就简单的选择,去利用现有的比较好的一些state,这这是一个trade off f,第二个问题叫叫delay feedback,就是在R中整个对于系统的反馈呃,一般来说是相对系数,而且是比较延迟的,也就是说呃呃呃比如说当我当我们下盘围棋,我们事后可能会说这个呃这局我输了,我输的主要原因可能因为我中间的某一首,比如说我第100步的第100首,这一首下错了,就是人,我们把整个整个围棋,围棋这一局围棋输了的原因,归结为其中某一步下的不好的,这个过程叫credit assemon,就是我们把这个呃对或者不对的原因,具体指到直接引发这个呃这个结果的内部,但这个事情其实是非常不平凡的,在R里面,我当我们有一个非常很大的horizon,而且我们的feedback是是delay,就是当你做错了之后,环境并不直接告诉你做错了呃,呃你中间做错了某一步,可能会在随后的执行中,逐渐的逐渐的进入了一个不太好的区域,然后在最终得到了一个不好的一个feedback,就这个feedback它是它是有延迟的,呃,这个延迟就会导致这个credit sement,这个问题变得第一个很重要的challenge,然后呃最后一个CHALLEN值就是整个formulation,它可能会存在一些问题,就是我们刚刚也看到了整个formulation,就是关于MDPLL的这个formulation,它是非常general的,像general的话,就意味着原则上它是可以apply到很多,很多的情形,但实际上在一些reword的一些setting中,这个MVP具体应该怎么定义,在很多时候是并不是一个很平凡的问题,就比如说在大模型中的MDP,目前这么定义是否合理,就光是呃光是这个问题,目前在学术界就依然有些讨论,然后呃当然如果我们想要放在现实,现实生活中应用的话,我们更关心的一般来说是feedback的信号,在RL的语言中,就是这个,如如果signal有没以什么样的方式,能够真正获得一个比较可靠的一个,如果signal就这个是把LL应用在现实生活中,要面临的在formulation上的问题,整体上就是呃关于R的差异,主要是这几个方面,那我们接下来额要进行一些数学的一些推导,然后呃通过这些推导,我们可以看到就是在RL的这个MDP,MDP当中,其实已经蕴含了很多很多信息,然后我们通过我们只要去稍微分析一下,这些信息,我们就可能够得到一些很关键的,可以直接设计LL算法的一些技术呃,然后呃这部分内容有很多东西,都跟王老师之前提到的那个belman equation呃,就是跟当时做ultimate control,这方面的一些工作是相关的,但是呃大家也不需要有一些背景知识,我们直接去看这个式子就可以,他的呃形式上看起来可能有一定复杂性,但是它的整个intuition还是非常可略的,我们之前提到L的目标是希望去optimize呃,这上面这个式子,它也都是随着这个累积期望内的一个呃,呃整个的combative return,但是我们可以把这个trajectory的这个sample,给去去展开一下,然后我我们展开完之后就会发现,首先是这个S0,S0,它是跟初始分步骤零相关的,然后在S0内部会有A0,AA0是取决于当前的policy depend on,S0的预测结果,然后有SNA0之后换电会给二零,二零之后就就是sex state,sex state是S1S1,依然是depend on呃,呃就是S0到A0的这个TRITION,然后这个过程可以还可以继续往后无限展开,就当我们这么展开的话,我们就会发现,其实呃这个展开过程中,是有很多的重复在里面的,就是这里是有所谓的子结构在里面,就是因为毕竟在mdp condition的,每一步和每一步都是相似的,所以呃我们如果能够呃更更好的去capture,这个子结构中间的一些信息,它就很可能会非常有利于我们去分析,整个MVP的这个过程,所以我们接下来可以形式上做这样,这样两个定义,就是呃given当前的一个POY派,我们可以定义它的value function,它的它的value function是gin的,On s,它描述的是我如果目前呃已经处于S这个state,然后随后我就直接去follow这样1policy,那那么我呢根据期望我能够获得多少reword,它的形式上,也也就是当我们的这个abroad是the ror的,这个S0等于当前S的时候,我们随后能够获得的一个累计期望的数量,就是我们可以定义这样的一个V派S,它衡量的其实就相当于是呃所有的state,地下state会有好有坏,我们去衡量呃,呃哪些state是好的,哪些是坏的,他们具体有多好有多坏,就是我们可以这样定义一个value function,他是depend on s,然后相似的我们可以,我们也可以depend一个q function,它也叫A1action value function,它就depend depend on s和A的这样一个突破,它描述的就是当我们已经在状态S了,我现在决定去take action a,那么我take完这一步之后,整体上我接着follow当前的policy,那么我能够获得什么样的reword,那它的形式也就也是很直观,就是首先我们会会获得一个立即就位的啊,然后其次这个环境会传递到下一个state as prime,然后在as prime当中,我们就继续继续之前的定义,当整个RO的去呃,使自as prime的情况下,我们有可按照期望能够获得多少的reward,所以我们现在相当于是定义了两个value function,就是这两个方程都是DEPON派的,就在当呃根据当前的policy,我们可以用拿用这两个记号来描述这policy,对于这些S,所以对于S以及s a pet的一些preference,然后哦我们就刚刚大家可能也已经看到,就是当我们定义这个vs的时候,vs内部其实又出现了跟V类似的结构,就像这个Q派SA里面也出现了,类似于跟Q1样结构一样,就是这整个MDP安装roll的过程中,是有非常丰富的一些子结构在里面,所以它也有很多呃自举的性质在里面,这里呃,我最上面的这两个式子,就是所谓的标man equation呃,它描述的是value function和q function跟自己呃呃怎么迭代的,用value value function来描述自己,比如说我们先看第一行,第一行看看是不是嗯休息20分钟,10.20再继续吧,嗯对对,这个可能还是是这样安排的吧,呃是呃,王老师,尚老师,大家可以休息20分钟,嗯10.20我们再继续,好嘞,嗯嗯好的