对，自己是很明确地认为必须是小的模型才能够
但是会不会被MoE架构偷家呢，可能的。

概括所有的token形成model，然后通过model预测下一个model应该是什么



LGRPO(θ)=−1G∑i=1G1|oi|∑t=1|oi|[πθ(oi,t∣q,oi,<t)[πθ(oi,t∣q,oi,<t)]no gradA^i,t−βDKL[πθ‖πref]] 

\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \frac{\pi_\theta(o_{i,t} \mid q, o_{i,< t})}{\left[\pi_\theta(o_{i,t} \mid q, o_{i,< t})\right]_{\text{no grad}}} \hat{A}_{i,t} - \beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \right]  
\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \frac{\pi_\theta(o_{i,t} \mid q, o_{i,< t})}{\left[\pi_\theta(o_{i,t} \mid q, o_{i,< t})\right]_{\text{no grad}}} \hat{A}_{i,t} - \beta \mathbb{D}_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right] \right]  

但是
真的感觉有问题的，比如余弦相似度什么的。
这样下来求出来的东西只可能是整个语言的分布。

一个，联想，另一个，相通的，变换，上下文，变换
如是多次之后

那么也就是CoT基本上是正解
MoE基本上是正解
Transformer不是正解。

遠いかも


树状连接，也就是说语言可以直接接续到绘画之类的。
DiT。不是恨聪明的做法但是感觉

应该能做分立的和全能的。姑且还是看code吧。


希德尼娅 发表于 2025-2-8 15:35
低参数模型通过蒸馏能达到高参数模型的效果实在有些反逻辑

高密度小模型反杀参数更大的模型是过去一年的主旋律


分布炸了


希德尼娅 发表于 2025-2-8 15:35
低参数模型通过蒸馏能达到高参数模型的效果实在有些反逻辑

不是刚出现了50美元训练成本的S1-32b在Math500、AIME24、GPQA，3个科目上打平了R1和o1么。


qratosones1337 发表于 2025-2-8 17:00
高密度小模型反杀参数更大的模型是过去一年的主旋律

但是ds的671b不就打败了o1的300b，说明参数少还是不行吧


希德尼娅 发表于 2025-2-8 15:35
低参数模型通过蒸馏能达到高参数模型的效果实在有些反逻辑

并不吧，按照微软的论文透露，openai 和 claude 参数量也没有巨高，数据本身质量也是一个重要变量


希德尼娅 发表于 2025-2-9 11:05
但是ds的671b不就打败了o1的300b，说明参数少还是不行吧

o1参数多少现在还没有定论，另外DSV3作为MoE模型每次激活参数只有33B


希德尼娅 发表于 2025-2-9 11:05
但是ds的671b不就打败了o1的300b，说明参数少还是不行吧

作为知名CloseAI，OAI早就停止公布参数量了，有多少都靠猜。



虽然很想得到AI很牛逼的结论
但是现在的应该确实不行。
但是可能性已经放在这里了。
